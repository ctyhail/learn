[TOC]

# 机器学习

经典定义：利用经验改善自身的系统  

## 数据集

**训练数据与测试数据**要分开

设数据集为D，训练集S和测试集T往往来自数据集，但怎样从D中划分S和T会最终模型产生影响。以下是几种方法划分数据：

### 训练集

训练集是机器学习模型用于训练和学习。一般来说，训练集来自于原始数据集，用于训练模型的参数。模型通过训练集来学习数据的特征，并产生一个模型，以便在后续预测中使用。

### 测试集

测试集用于评估模型的最终性能，通常来自于原始数据集，与训练集和验证集互斥。测试集作用是评估模型在未见过的数据上的性能，并判断模型是否足够准确。

### 验证集

验证集用于评估模型性能，通常来自于原始数据集，用于在训练过程中调整模型的参数和超参数，以提高模型的性能。验证集的作用是帮助开发人员调整模型，避免模型欠拟合和过拟合。

### 划分数据集的方法

#### 留出法

##### 定义

将数据集D划为两个互斥的集合，其中训练集为S，测试集为T，S与T之间没有交集。在S训练出模型后，由T来评估其测试误差和模型性能。

##### 数据的划分

但值得注意的是，S和T的划分要保证数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终的结果产生影响。

另一个值得注意的问题是，即使给定S与T的比例，也存在多种划分方式对D进行分割。不同的划分方式训练出的模型往往也不同。因此，单次留出法得到的结果往往不够稳定可靠（缺点不足）。应对方法：采用如若干次随机划分和重复试验评估后平均值作为留出法的评估结果。

##### 数据的大小

S太大，T太小，训练出的模型评估结果可能不够稳定准确。但若T过大，训练出的模型与用D训练出的模型相差可能过大，从而降低了评估结果的保真（fidelity)。一般来说，常见做法是将大约2/3~4/5的数据用于训练，剩下的用于测试。

#### 交叉验证法

##### 定义

将D划分为k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。然后，每次用k-1个子集的并集做训练集，余下的自己做测试集，这样可以得到k组训练/测试集，可进行k次训练和测试，最终返回的是k个测试结果的均值。

##### 特殊之处

交叉验证法评估的结果的稳定性和保真性在很大程度上取决于k的取值，因此这种方法也被称为“k折交叉验证”

##### 数据的划分

与留出法相似，数据集D划分为k个子集同样存在多种划分方式，因此，也要随机使用不同的的划分方法重复p次，最终的评估结果是p次k折交叉验证结果的均值。

##### 留一法

留一法是交叉验证法的一个特例。假定D中有m个样本，令k=m，也就是说子集的个数与样本的个数相等，一个子集包含一个样本。留一法使用的训练集与初始数据集之相差一个样本，这就使留一法被实际评估的模型与期望评估的用D训练出的模型和相似。因此，留一法的评估结果往往被认为比较准确。

但留一法有一个较大的缺陷：当数据集较大时，训练m个模型的时间复杂度是难以承受的。况且，留一法的估计结果未必永远比其他评估结果稳定。

#### 自助法（可重复采样/可放回采样）

在留出法和交叉验证法中，由于总保留了一部分数据用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差，而自助法是一个较好的解决方法。

##### 定义

给定含m个样本的数据集D，对它进行采样产生数据集D'：每次随机从D中挑选一个样本，将其拷贝并放入D'，再将该样本放回D，使该样本在下次采样中仍有可能被采样；这个过程重复执行m次，就可以得到一个包含m个样本的数据集D'。

##### 测试结果

显然，可能回有一部分样本在D'中多次出现，可能会有一部分样本不出现。做一个简单的估计，样本在m次的采样中始终不被采样的概率为<img src="C:/Users/20247/Downloads/QianJianTec1699455057553.jpg" alt="QianJianTec1699455057553" style="zoom:2%;" />取极限为：

<img src="C:/Users/20247/Downloads/QianJianTec1699454811263.jpg" alt="QianJianTec1699454811263" style="zoom:8%;" />

通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集。可将D'用作训练集，D/D'用作测试集；这样，实际评估模型和期望评估模型都是使用m和训练样本，但仍有数据总量的1/3没有在训练集中出现的的样本用于测试，这样的测试结果被称为“包外估计”。

##### 优点

自助法在数据集较小，难以有效划分训练/测试集时很有用。此外，自助法能从初始数据集中产生多个不同的训练集，这对**集成学习**等方法有很大的好处。

##### 缺点

自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，当初始数据量足够时，留出法和交叉验证法更常用。

## numpy

numpy是Python语言的一个第三方库，其支持大量高维度数组和矩阵的运算，还有针对数组运算提供大量数学函数，对机器学习十分有用。

### 导入numpy库

```
import numpy as np
```

导入库并用`as`将`numpy`简化为`np`。便于后面的使用。

### 创建数组

#### 通过列表创建一维或多维数组（矩阵）

```
np.array([1,2,3])#创建一维数组
np.array([1,2,3],[4,5,6])#创建二维数组
以此推类
```

#### 与数组有关的函数

##### full函数

```
a=np.full((3,4),2)
```

创建任意大小的数组并填充数字，`print(a)`的结果为：

```
array([[2, 2, 2, 2],
   [2, 2, 2, 2],
   [2, 2, 2, 2]])
```

##### arange函数

创建等差数组：

```
a1=np.arange(5)#创建一维等差数组
a2=np.arange(6).reshape(2,3)#创建二维等差数组
```

`print(a1)`和`print(a2)`的结果为：

```
array([0, 1, 2, 3, 4])
array([[0, 1, 2],
   [3, 4, 5]])
```

##### eye函数

创建单位矩阵（二维数组）。

```
np.eye(3)
```

```
array([[1., 0., 0.],
   [0., 1., 0.],
   [0., 0., 1.]])
```

##### 随机函数

将随机数填入数组。

```
np.random.rand(2,3)#创建随机二维数组
np.random.randint(5,size=(2,3))#创建随机整数二维数组（数值小于5）
```

```
array([[0.38257919, 0.63590106, 0.64884528],
   [0.09064574, 0.32850939, 0.94661844]])
array([[2, 3, 4],
   [3, 2, 4]])
```

### 数组与矩阵的运算

#### 运算函数

##### sum函数

将数组里每个元素相加求和

```
a=np.array([1,2],[3,4])
print(sum(a))
```

```
10
```

还有其他用法：

```
np.sum(a,axis=0)#axis=0 表示对每一列求和
np.sum(a,axis=1)#axis=1 表示对每一行求和
```

```
array([4, 6])
array([3, 7])
```

##### mean函数

求取平均值

```
np.mean(a)
```

```
2.5
```

##### tile函数

对数组进行复制移位创建一个新数组

```
a=np.array([1,2],[3,4])
b1=np.tile(a,(1,2))
b2=np.tile(a,(2,1))
```

`print(b1)`和`print(b2)`结果为：

```
array([[1, 2, 1, 2],
   [3, 4, 3, 4]])
array([[1, 2],
   [3, 4],
   [1, 2],
   [3, 4]])
```

##### argsort函数



## 卷积神经网络（CNN)

卷积神经网络分为输入层，卷积层，池化层，全连接层，输出层

- 输入层：输入图像等信息
- 卷积层：用来提取图像的底层特征
- 池化层：防止过拟合，将数据维度减小
- 全连接层：汇总卷积层和池化层得到的图像的底层特征和信息
- 输出层：根据全连接层的信息得到概率最大的结果

### 输入层

输入层是输入图像等信息，因为卷积神经网络主要处理的是图像相关的内容。输入层要将图像转化为计算机能理解的格式，所以对于输入图像，一般将其转为二维矩阵，矩阵中的每一个值对应图像每一个像素点的像素值，比如：

![请添加图片描述](https://img-blog.csdnimg.cn/9362dad9931443e29289610b952b4e8f.gif#pic_center)

上图又称为灰度图像，因为其每一个像素值的范围是0~255（由纯黑色到纯白色），表示其颜色强弱程度。另外还有黑白图像，每个像素值要么是0（表示纯黑色），要么是255（表示纯白色）。我们日常生活中最常见的就是RGB图像，有三个通道，分别是红色、绿色、蓝色。每个通道的每个像素值的范围也是0~255，表示其每个像素的颜色强弱。但是我们日常处理的基本都是灰度图像，因为比较好操作（值范围较小，颜色较单一），有些RGB图像在输入给神经网络之前也被转化为灰度图像，也是为了方便计算，否则三个通道的像素一起处理计算量非常大。当然，随着计算机性能的高速发展，现在有些神经网络也可以处理三通道的RGB图像。（复制粘贴）

### 卷积层

#### 卷积核（Convolution Kernel）滤波器

卷积核判断事物的主要特征，也是一个二维矩阵，其大小等于或小于输入图像的矩阵大小

卷积核可以有多个，取决于事物的主要特征。当然有几个卷积核就有几个特征图。

#### 卷积操作

提取图片二维矩阵的特征。卷积操作会为存在特征的等等区域确定一个高值，否则为低值。这个过程需要计算图像二维矩阵与卷积核的乘积值来确定，得到的也会是一个矩阵。卷积核会在图像二维矩阵上不停地移动，每一次移动都进行依次乘积求和。通过整个卷积操作可以得到一个新的二维矩阵，这个二维矩阵由被称为特征图（Feature Map），一般来说小于输入图像的二维矩阵大小。

假设我们现在的输入图片是一个人的脑袋，而人的眼睛是我们需要提取的特征，那么我们就将人的眼睛作为卷积核，通过在人的脑袋的图片上移动来确定哪里是眼睛，这个过程如下所示：

![请添加图片描述](https://img-blog.csdnimg.cn/07e8e669b3cb4ec5825989a8b22eab73.gif#pic_center)

卷积过程为：

![请添加图片描述](https://img-blog.csdnimg.cn/8dac00a7d8954ac9930b64dc6e146de6.gif#pic_center)

上方为特征图，下方为输入图像矩阵。

#### 解决特征丢失（padding）

输入图像矩阵的边缘很少被计算，这样会导致计算结果不准确，得到的特征图也会丢失边缘特征，最终导致特征提取不准确。为了解决这个问题，可以在初始输入图像矩阵周围拓展一圈或几圈。

扩展一圈：

![请添加图片描述](https://img-blog.csdnimg.cn/7a3fe6889ef640b196a5bb93f9bcaf74.gif#pic_center)

扩展两圈：

![请添加图片描述](https://img-blog.csdnimg.cn/fa1524ec74db461f9a041984bb482b04.gif#pic_center)

#### 拓宽的方法

```python
import numpy as np  
  
# 输入图像矩阵  
image = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  
  
# 定义拓宽的宽度  
padding_width = 1  
  
# 进行拓宽操作，使用0进行填充  
padded_image = np.pad(image, pad_width=padding_width, mode='constant', constant_values=0)  
  
# 输出拓宽后的图像矩阵  
print(padded_image)
```

这个示例中，我们使用`np.pad`函数对输入的图像矩阵进行拓宽。`pad_width`参数指定了要添加的填充像素的数量，`mode`参数指定了填充的方式（这里使用常量填充，即填充0），`constant_values`参数指定了要填充的值（这里填充0）。输出结果是一个拓宽后的图像矩阵。

### 池化层

#### 设置池化层的目的

特征图的个数取决于卷积核的个数，当特征图过多时，意味得到的特征也很多，但有些特征并不是我们需要的，多余的特征会带来两个问题：

- 过拟合
- 维度过高

池化层的作用就是对特征图在进行特征提取，将最有代表性的特征提取出来，可以起到减少过拟合和降低维度的作用。

#### 池化方法

- 最大池化

  最大池化就是每次取正方形中所有值的最大值，这个最大值也就相当于当前位置最具有代表性的特征

  ![请添加图片描述](https://img-blog.csdnimg.cn/299741f0702b4b42bbf0905ebb1e6966.png#pic_center)

  这里有几个参数需要说明一下：
        ① kernel_size = 2：池化过程使用的正方形尺寸是2×2，如果是在卷积的过程中就说明卷积核的大小是2×2
        ② stride = 2：每次正方形移动两个位置（从左到右，从上到下），这个过程其实和卷积的操作过程一样
        ③ padding = 0：这个之前介绍过，如果此值为0，说明没有进行拓展

- 平均池化

  平均池化就是取此正方形区域中所有值的平均值，考虑到每个位置的值对于此处特征的影响

  ![请添加图片描述](https://img-blog.csdnimg.cn/6db2b5cfd0d34ebf8b857b78a6fa98db.png#pic_center)

  

  #### 池化层的好处

  - 减少参数，保留原图像的原始特征
  - 防止或拟合
  - 为卷积神经网络带来平移不变性

  ### 全连接层

  目前还算了解，暂不做笔记

  ### 输出层

  计算最终概率

## scale

scale的作用是对参数进行适当的缩放，以避免初始化权重过大或过小的问题

scale的计算基于HE初始化(He initialization)的思想。He初始化是常见的权重初始化的方法，特别适用于使用ReLU等激活函数的神经网络 。



### Batch Normalzation(批量归一化)





